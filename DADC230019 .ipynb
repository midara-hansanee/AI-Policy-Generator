{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zVnUpxX2RSwH",
        "outputId": "1c99a804-c51d-4eca-b2c2-b4b8751f1b62"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the nltk module before calling its functions\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Download punkt_tab to resolve the error\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AoydPCD3RSyW",
        "outputId": "61dbef71-5b4a-4c79-92ac-f89cd6f4d238"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN0qcTzJSZrA",
        "outputId": "bfabc323-3ba9-4e01-9afa-23871cb4081e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import fitz  # PyMuPDF for PDF handling\n",
        "import nltk\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Load environment variables (for API key)\n",
        "load_dotenv() # Call the load_dotenv function to load environment variables from .env file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6npg02hRS0l",
        "outputId": "93588da1-09ce-42cb-e739-7d36d4090a6e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load environment variables (for storing API key)\n",
        "load_dotenv()  # Call the load_dotenv function to load environment variables from .env file\n",
        "\n",
        "# Load API Key securely\n",
        "API_KEY = os.getenv(\"AIzaSyAtVtbDr29PLqhlskm5NLQX5ocVjgYge4o\")\n",
        "\n",
        "# If API_KEY is missing, use a fallback (REPLACE with your actual key)\n",
        "if not API_KEY:\n",
        "    API_KEY = \"AIzaSyAtVtbDr29PLqhlskm5NLQX5ocVjgYge4o\"  # Replace with your valid key\n",
        "\n",
        "assert API_KEY, \"ERROR: Gemini API Key is missing or incorrect\"\n",
        "\n",
        "# Configure Gemini API\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# Initialize model\n",
        "model = genai.GenerativeModel(\"gemini-1.5-pro-latest\")\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download([\"stopwords\", \"punkt\", \"wordnet\"])\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)  # Open PDF\n",
        "        text = \"\\n\".join(page.get_text() for page in doc)\n",
        "        doc.close()\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Initialize NLP tools\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "unnecessary_words = {\"etc\", \"e.t.c\", \"eg\", \"i.e\", \"viz\"}\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r\"[^a-zA-Z0-9.%\\s]\", \"\", text)  # Preserve numbers & percentages\n",
        "    words = word_tokenize(text)  # Tokenize words\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words and word not in unnecessary_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Get PDF path\n",
        "pdf_path = input(\"Enter the path to the policy PDF file: \")\n",
        "\n",
        "# Extract and clean PDF content\n",
        "pdf_text = extract_text_from_pdf(pdf_path)\n",
        "if not pdf_text:\n",
        "    print(\"No text extracted from the PDF. Check the file path and content.\")\n",
        "    exit()\n",
        "\n",
        "cleaned_text = clean_text(pdf_text)\n",
        "\n",
        "# Trim text for Gemini API (to avoid exceeding limits)\n",
        "cleaned_text = cleaned_text[:10000]  # Limit input to 10,000 characters\n",
        "\n",
        "# Generate a summary using Gemini\n",
        "summary_prompt = f\"\"\"\n",
        "Please provide a clear and concise summary of the following policy document:\n",
        "\n",
        "{cleaned_text}\n",
        "\"\"\"\n",
        "try:\n",
        "    summary_response = model.generate_content(summary_prompt)\n",
        "    print(\"\\n📄 Summary of the PDF Document:\\n\")\n",
        "    print(summary_response.text)\n",
        "except Exception as e:\n",
        "    print(f\"API Error: {e}\")\n",
        "\n",
        "# Generate a policy based on user inputs\n",
        "policy_type = input(\"\\nEnter the policy type (e.g., act, regulation, circular, gazette): \")\n",
        "scenario = input(\"Provide the scenario: \")\n",
        "\n",
        "policy_prompt = f\"\"\"\n",
        "Generate an economic policy from {policy_type} for the following scenario:\n",
        "\n",
        "Scenario: {scenario}\n",
        "\n",
        "Reference Policy Content:\n",
        "{cleaned_text}\n",
        "\n",
        "Ensure the generated policy is well-structured, clear, and follows professional standards.\n",
        "\"\"\"\n",
        "try:\n",
        "    policy_response = model.generate_content(policy_prompt)\n",
        "    print(\"\\n📜 Generated Policy:\\n\")\n",
        "    print(policy_response.text)\n",
        "except Exception as e:\n",
        "    print(f\"API Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IaE7UkwYrN9t",
        "outputId": "48b77225-4b23-4df3-dff0-a894aeff5fb0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path to the policy PDF file: /content/teejay-policy-statement-dma.pdf\n",
            "\n",
            "📄 Summary of the PDF Document:\n",
            "\n",
            "Teejay Lanka PLC, a Sri Lankan textile manufacturer, aims to be a preferred fabric solutions provider through sustainable practices.  Their GRI 33 Management Approach disclosure outlines their commitment to economic, environmental, and social responsibility (ESG).  \n",
            "\n",
            "**Economic Performance:** They focus on delivering value to stakeholders, adhering to corporate governance, and complying with local regulations. They monitor financial performance, manage risk, and have anti-corruption policies in place, including a whistleblower policy. They track KPIs like minimum wage payments and compliance with statutory provisions.\n",
            "\n",
            "**Environmental Stewardship:**  Committed to minimizing environmental impact, they adhere to ISO 14001:2015 and aim for net-zero emissions by 2050. They monitor energy and water consumption, emissions, waste generation, and effluent discharge.\n",
            "\n",
            "**Social Responsibility (implied):**  While not explicitly detailed in this excerpt, the document mentions stakeholder engagement and responsible labor practices, suggesting a broader commitment to social responsibility.\n",
            "\n",
            "Teejay Lanka uses a best-in-class process approach to address ESG risks, monitors performance through KPIs, and reports findings to senior management.  The management approach is reviewed annually, and the company obtains independent third-party assurance for its sustainability reporting.  A steering committee oversees ESG management and policy formulation.\n",
            "\n",
            "\n",
            "Enter the policy type (e.g., act, regulation, circular, gazette): circular\n",
            "Provide the scenario: finance\n",
            "\n",
            "📜 Generated Policy:\n",
            "\n",
            "## Teejay Lanka PLC: Economic Performance Policy\n",
            "\n",
            "**1. Introduction**\n",
            "\n",
            "Teejay Lanka PLC (\"Teejay\" or \"the Company\") is committed to delivering sustainable economic value addition to all stakeholders while upholding the highest standards of corporate governance, environmental stewardship, and social responsibility. This policy outlines Teejay's approach to economic performance, aligned with the Global Reporting Initiative (GRI) standards (GRI 201: Economic Performance, GRI 205: Anti-corruption) and contributing to the Company's overall sustainability strategy.\n",
            "\n",
            "**2. Policy Objectives**\n",
            "\n",
            "This policy aims to:\n",
            "\n",
            "* Generate sustainable economic value for shareholders, employees, customers, suppliers, and the wider community.\n",
            "* Ensure compliance with all relevant local and international regulations, including tax laws and corporate governance principles.\n",
            "* Promote transparency and accountability in financial reporting and business operations.\n",
            "* Implement robust risk management processes to mitigate financial and operational risks.\n",
            "* Foster a culture of integrity and ethical conduct, with a zero-tolerance approach to corruption.\n",
            "* Enhance stakeholder engagement and communication regarding economic performance.\n",
            "\n",
            "**3. Key Principles**\n",
            "\n",
            "* **Value Creation:**  Teejay strives to create long-term value through sustainable business practices that benefit all stakeholders.\n",
            "* **Corporate Governance:** The Company adheres to best practices in corporate governance, including sound financial management, stringent internal controls, and a robust risk management framework.\n",
            "* **Transparency & Accountability:**  Teejay is committed to transparent financial reporting and open communication with stakeholders regarding economic performance.\n",
            "* **Integrity & Anti-Corruption:** The Company maintains a zero-tolerance policy towards corruption and promotes a culture of ethical conduct throughout the organization.\n",
            "* **Compliance:**  Teejay adheres to all applicable laws and regulations in the countries where it operates.\n",
            "\n",
            "**4. Implementation**\n",
            "\n",
            "* **Financial Management:** Teejay maintains rigorous financial processes, including internal audits and enterprise risk management, overseen by the Board Audit Committee and the Head of Risk Management.\n",
            "* **Risk Management:**  Operational, financial, and non-financial risks are identified and managed through a comprehensive enterprise risk management (ERM) process.\n",
            "* **Anti-Corruption:** Teejay has implemented a Whistleblower Policy and a Code of Conduct to prevent and address instances of corruption.  All employees are required to adhere to the Code of Conduct and are encouraged to report any suspected violations.\n",
            "* **Stakeholder Engagement:**  Teejay engages regularly with its stakeholders to understand their perspectives on economic performance and to ensure their interests are considered in decision-making processes.\n",
            "* **Performance Monitoring:** Key Sustainability Performance Indicators (KSPIs) related to economic performance, such as payment of minimum wages, EPF/ETF contributions, and incidence of corruption, are tracked and reported regularly.\n",
            "\n",
            "\n",
            "**5.  Reporting and Review**\n",
            "\n",
            "*  Teejay's economic performance is reported periodically to senior management and the Board of Directors.\n",
            "*  This policy and its effectiveness will be reviewed annually by the senior management team, with guidance from the Chief Financial Officer and the Sustainability Division.  \n",
            "*  The ESG Management Steering Committee will oversee the implementation of this policy and will formulate necessary procedures.\n",
            "\n",
            "**6. Responsibilities**\n",
            "\n",
            "* The Board of Directors is responsible for overseeing the implementation of this policy.\n",
            "* The Chief Financial Officer and the Sustainability Division are responsible for providing guidance and support for the implementation of this policy.\n",
            "* All employees are responsible for adhering to this policy and reporting any suspected violations.\n",
            "\n",
            "\n",
            "**7.  Communication**\n",
            "\n",
            "This policy will be communicated to all employees and relevant stakeholders through the company intranet, website, and other appropriate channels.\n",
            "\n",
            "\n",
            "This policy forms part of Teejay's overall commitment to sustainability and responsible business practices. It is designed to ensure that the Company creates long-term economic value while contributing positively to society and the environment.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio transformers nltk torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "4d_RUPhZRS5F",
        "outputId": "9a28df54-40fa-4a47-c9e4-29ee53f26fe2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markupsafe, groovy, ffmpy, aiofiles, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.0 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "markupsafe"
                ]
              },
              "id": "47336af7ddbc4d8bbabf3300c0400bb5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import gradio as gr# Gradio is used for building user interfaces for machine learning models\n",
        "import nltk# NLTK is a natural language processing library\n",
        "import torch# PyTorch is a deep learning library, used here for transformer models\n",
        "from transformers import pipeline# Pipeline from Hugging Face Transformers for easy model use\n",
        "\n",
        "nltk.download('punkt')# Punkt tokenizer is needed for tokenizing text in NLTK\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gYiLrg6RRS7U",
        "outputId": "a4bbf109-81c3-4421-9822-977ca721f663"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a summarization pipeline using the \"facebook/bart-large-cnn\" pre-trained model\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "# \"facebook/bart-large-cnn\" is a pre-trained BART model fine-tuned for the summarization task\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FH-AHG9q17YF",
        "outputId": "cfebacd3-8aec-41d9-89c7-b802e1bf9222"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer from Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Tokenizer converts text into tokens\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")  # Load the GPT-2 model for causal language modeling\n",
        "\n",
        "# Function to generate policy text based on a given scenario\n",
        "def generate_policy(scenario):\n",
        "    # Encode the input scenario into token IDs that the model can process\n",
        "    input_ids = tokenizer.encode(scenario, return_tensors=\"pt\")  # Convert text to tensor format\n",
        "\n",
        "    # Generate text (policy) by feeding the encoded input into the model\n",
        "    output = model.generate(input_ids, max_length=200)  # Set a max length for the generated text\n",
        "\n",
        "    # Decode the generated token IDs back into human-readable text and return the policy\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)  # Decode and skip special tokens (e.g., <pad>, <eos>)\n"
      ],
      "metadata": {
        "id": "VybI-Zsa2MWO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text):\n",
        "    # Check if the input text is too short for summarization\n",
        "    if len(text) < 100:\n",
        "        return \"Text is too short for summarization. Please provide a longer document.\"\n",
        "\n",
        "    # Use the pre-defined summarizer pipeline to generate a summary\n",
        "    summary = summarizer(text, max_length=500, min_length=50, do_sample=False)\n",
        "    # Return the summary text generated by the summarizer\n",
        "    return summary[0]['summary_text']\n"
      ],
      "metadata": {
        "id": "QNRbcc0V2ZA7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(file):\n",
        "    # Read the content of the file and decode it to a string using UTF-8 encoding\n",
        "    text = file.read().decode(\"utf-8\")\n",
        "\n",
        "    # Pass the text through the summarize_text function to generate a summary\n",
        "    return summarize_text(text)\n"
      ],
      "metadata": {
        "id": "ig5zbPrO2gNv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import PyPDF2\n",
        "import re\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load AI models\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")  # Summarization model\n",
        "policy_generator = pipeline(\"text-generation\", model=\"gpt2\")  # Policy generation model\n",
        "\n",
        "def process_file(file):\n",
        "    \"\"\"Extract text from a PDF file and generate a structured, point-based summary.\"\"\"\n",
        "    if file is None:\n",
        "        return \"No file uploaded.\"\n",
        "\n",
        "    try:\n",
        "        with open(file.name, \"rb\") as f:\n",
        "            pdf_reader = PyPDF2.PdfReader(f)\n",
        "            text = \"\\n\".join([page.extract_text() for page in pdf_reader.pages if page.extract_text()])\n",
        "\n",
        "        if not text.strip():\n",
        "            return \"No readable text found in the document.\"\n",
        "\n",
        "        # Summarize using NLP model\n",
        "        summary = summarizer(text[:1024], max_length=300, min_length=100, do_sample=False)  # Processing limit for BART\n",
        "        summarized_text = summary[0]['summary_text']\n",
        "\n",
        "        # Extract key points using simple heuristics\n",
        "        points = re.split(r\"\\n\\d+\\.|\\n-|\\n•\", summarized_text)  # Splitting based on bullet indicators\n",
        "        points = [p.strip() for p in points if len(p.strip()) > 10]  # Removing short items\n",
        "\n",
        "        # Format as bullet points\n",
        "        formatted_summary = \"\\n\".join([f\"• {p}\" for p in points]) if points else summarized_text\n",
        "        return formatted_summary\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing file: {str(e)}\"\n",
        "\n",
        "def generate_policy(scenario):\n",
        "    \"\"\"Generate a structured, AI-powered policy based on the given scenario.\"\"\"\n",
        "    if not scenario.strip():\n",
        "        return \"Please enter a valid policy scenario.\"\n",
        "\n",
        "    response = policy_generator(scenario, max_length=300, num_return_sequences=1, do_sample=True)[0][\"generated_text\"]\n",
        "\n",
        "    # Structuring the policy output\n",
        "    structured_policy = f\"📜 Policy Title:** {scenario}\\n\\n\"\n",
        "    structured_policy += \"\\n\".join([f\"• {line.strip()}\" for line in response.split('. ') if line.strip()])\n",
        "\n",
        "    return structured_policy\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks() as app:\n",
        "    gr.HTML(\n",
        "        \"\"\"\n",
        "        <style>\n",
        "            @import url('https://fonts.googleapis.com/css2?family=Poppins:wght@400;600&display=swap');\n",
        "            body {\n",
        "                font-family: 'Poppins', sans-serif;\n",
        "                background-color: #f0f4f8;\n",
        "            }\n",
        "            .title-box {\n",
        "                background-color: #5E72EB; /* Vibrant blue */\n",
        "                color: white;\n",
        "                padding: 20px;\n",
        "                border-radius: 10px;\n",
        "                text-align: center;\n",
        "                font-size: 36px;\n",
        "                font-weight: 600;\n",
        "                margin: 20px auto;\n",
        "                box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\n",
        "            }\n",
        "            .section {\n",
        "                background-color: white;\n",
        "                padding: 25px;\n",
        "                border-radius: 10px;\n",
        "                box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1);\n",
        "                margin: 20px 0;\n",
        "            }\n",
        "            .section h3 {\n",
        "                color: #5E72EB; /* Vibrant blue for headings */\n",
        "                font-size: 24px;\n",
        "                margin-bottom: 15px;\n",
        "                font-weight: 600;\n",
        "            }\n",
        "            .button-primary {\n",
        "                background-color: #5E72EB !important;\n",
        "                color: white !important;\n",
        "                border: none !important;\n",
        "                padding: 12px 24px !important;\n",
        "                border-radius: 8px !important;\n",
        "                font-size: 16px !important;\n",
        "                transition: background-color 0.3s ease;\n",
        "            }\n",
        "            .button-primary:hover {\n",
        "                background-color: #4A5FCC !important;\n",
        "            }\n",
        "            .textbox {\n",
        "                border-radius: 8px !important;\n",
        "                border: 1px solid #ddd !important;\n",
        "                padding: 12px !important;\n",
        "                font-size: 14px !important;\n",
        "                background-color: #f9f9f9;\n",
        "            }\n",
        "            .file-upload {\n",
        "                background-color: #f9f9f9;\n",
        "                padding: 15px;\n",
        "                border-radius: 8px;\n",
        "                border: 2px dashed #5E72EB;\n",
        "                text-align: center;\n",
        "            }\n",
        "            .file-upload label {\n",
        "                color: #5E72EB !important;\n",
        "                font-weight: 500;\n",
        "            }\n",
        "            .output-label {\n",
        "                font-size: 16px;\n",
        "                font-weight: 500;\n",
        "                color: #333;\n",
        "                margin-bottom: 10px;\n",
        "            }\n",
        "        </style>\n",
        "        <div class='title-box'>📜🤖 AI Policy Assistant</div>\n",
        "        <p style=\"text-align: center; color: #555; font-size: 16px;\">\n",
        "            Simplify policy analysis and generation with AI. Upload a document for summarization or describe a scenario to generate a policy.\n",
        "        </p>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Section 1: Document Summarization part\n",
        "    with gr.Column(elem_classes=\"section\"):\n",
        "        gr.HTML(\"<div class='section-box'><h3 style='color: #FFD700;'>📂 Upload & Summarize</h3>\")\n",
        "        file_input = gr.File(label=\"Upload a PDF file\", type=\"filepath\", elem_classes=\"file-upload\")\n",
        "        summarize_button = gr.Button(\"Generate Summary\", variant=\"primary\", elem_classes=\"button-primary\")\n",
        "        summary_output = gr.Textbox(label=\"Summary Output\", interactive=True, lines=10, elem_classes=\"textbox\")\n",
        "\n",
        "    # Section 2: Policy Generation part\n",
        "    with gr.Column(elem_classes=\"section\"):\n",
        "        gr.HTML(\"<div class='section-box'><h3 style='color: #00BFFF;'>✍ AI-Based Policy Generator</h3>\")\n",
        "        scenario_input = gr.Textbox(label=\"Describe your policy scenario\", elem_classes=\"textbox\")\n",
        "        generate_button = gr.Button(\"Generate Policy\", variant=\"primary\", elem_classes=\"button-primary\")\n",
        "        policy_output = gr.Textbox(label=\"Generated Policy\", interactive=True, lines=10, elem_classes=\"textbox\")\n",
        "\n",
        "    # Add Button actions\n",
        "    summarize_button.click(process_file, inputs=file_input, outputs=summary_output)\n",
        "    generate_button.click(generate_policy, inputs=scenario_input, outputs=policy_output)\n",
        "\n",
        "app.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "VT2M1oUv5iKB",
        "outputId": "2536543c-0331-446b-9b0f-8e236eb81693"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://e6b85046f5dbc4e772.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e6b85046f5dbc4e772.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ]
}